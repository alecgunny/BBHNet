import glob
import logging
import os
from collections.abc import Sequence
from typing import Optional

import h5py
import lightning.pytorch as pl
import torch
from luigi.contrib.s3 import S3Client
from train import augmentations as aug
from train.validation import get_timeslides

from aframe.architectures.preprocessing import PsdEstimator
from aframe.utils import x_per_y
from ml4gw.dataloading import Hdf5TimeSeriesDataset
from ml4gw.distributions import PowerLaw
from ml4gw.transforms import Whiten
from ml4gw.utils.slicing import sample_kernels, unfold_windows

Tensor = torch.Tensor


# TODO: using this right now because
# lightning.pytorch.utilities.CombinedLoader
# is not supported when calling `.fit`. Once
# this has been fixed in
# https://github.com/Lightning-AI/lightning/issues/16830,
# we should switch to using a CombinedLoader for validation
class ZippedDataset(torch.utils.data.IterableDataset):
    def __init__(self, *datasets):
        super().__init__()
        self.datasets = datasets

    def __len__(self):
        lengths = []
        for dset in self.datasets:
            try:
                lengths.append(len(dset))
            except Exception as e:
                raise e from None
        return min(lengths)

    def __iter__(self):
        return zip(*self.datasets)


class AframeDataset(pl.LightningDataModule):
    def __init__(
        self,
        # data loading args
        data_dir: str,
        ifos: Sequence[str],
        valid_frac: float,
        # preprocessing args
        batch_size: int,
        kernel_length: float,
        fduration: float,
        psd_length: float,
        # augmentation args
        waveform_prob: float,
        swap_frac: float,
        mute_frac: float,
        snr_thresh: float = 4,
        max_snr: float = 100,
        snr_alpha: float = 3,
        trigger_pad: float = 0,
        fftlength: Optional[float] = None,
        highpass: Optional[float] = None,
        # validation args
        valid_stride: Optional[float] = None,
        num_valid_views: int = 4,
        valid_livetime: float = (3600 * 12),
    ) -> None:
        super().__init__()
        self.save_hyperparameters()
        self.num_ifos = len(ifos)

        # infer the sample rate from the data
        with h5py.File(self.train_fnames[0], "r") as f:
            sample_rate = 1 / f[ifos[0]].attrs["dx"]
        self.sample_rate = sample_rate

        # set up some of the modules we'll need for
        # 1. Preprocessing
        fftlength = fftlength or kernel_length + fduration
        self.psd_estimator = PsdEstimator(
            kernel_length + fduration,
            sample_rate,
            fftlength,
            fast=highpass is not None,
            average="median",
        )
        self.whitener = Whiten(fduration, sample_rate, highpass)

        # 2. Data augmentation
        self.inverter = aug.SignalInverter(0.5)
        self.reverser = aug.SignalReverser(0.5)
        self.snr_sampler = PowerLaw(snr_thresh, max_snr, snr_alpha)
        self.projector = aug.WaveformProjector(ifos, sample_rate, highpass)
        self.swapper = aug.ChannelSwapper(swap_frac)
        self.muter = aug.ChannelMuter(mute_frac)
        self.waveform_sampler = None

    def get_local_device(self):
        if not self.trainer.device_ids:
            return "cpu"
        elif len(self.trainer.device_ids) == 1:
            return f"cuda:{self.trainer.device_ids[0]}"
        else:
            rank = os.getenv("LOCAL_RANK", "0")
            device_id = self.trainer.device_ids[int(rank)]
            return f"cuda:{device_id}"

    @property
    def sample_length(self) -> float:
        """Length of samples generated by datasets in seconds"""
        return (
            self.hparams.kernel_length
            + self.hparams.fduration
            + self.hparams.psd_length
        )

    @property
    def pad_size(self) -> int:
        """
        Number of samples away from edge of kernel to ensure
        that waveforms are injected at.
        """
        return int(self.hparams.trigger_pad * self.sample_rate)

    # TODO: should we come up with a more clever scheme for
    # breaking up our training and validation background data?
    @property
    def train_fnames(self) -> Sequence[str]:
        fnames = glob.glob(f"{self.hparams.data_dir}/background/*.hdf5")
        return sorted(fnames)[:-1]

    @property
    def valid_fnames(self) -> Sequence[str]:
        fnames = glob.glob(f"{self.hparams.data_dir}/background/*.hdf5")
        return sorted(fnames)[-1:]

    @property
    def steps_per_epoch(self) -> int:
        """
        Number of gradient updates between validation periods,
        taking into account number of devices currently being
        utilized for training implicitly through the number
        of waveforms on the local device.
        """

        # we don't load in waveforms until setup() gets called,
        # so in case we need this between init time and then,
        # grab the number of training waveforms explicitly.
        if self.waveform_sampler is None:
            train_frac = 1 - self.hparams.valid_frac
            with h5py.File(f"{self.hparams.data_dir}/signals.h5", "r") as f:
                num_waveforms = int(len(f["signals"]) * train_frac)
        else:
            # otherwise it should be saved as an attribute here
            num_waveforms = self.waveform_sampler.num_waveforms

        # multiply by 4 to account for the fact that
        # sky parameter sampling means we won't see
        # the same waveform the same way, and so we
        # technically have "more" data.
        waveforms_per_batch = (
            self.hparams.batch_size * self.hparams.waveform_prob
        )
        total_batches = int(4 * num_waveforms / waveforms_per_batch)
        return total_batches

    @property
    def val_batch_size(self):
        """Use larger batch sizes when we don't need gradients."""
        return int(4 * self.hparams.batch_size)

    @torch.no_grad()
    def project_val_waveforms(self, waveforms, dec, psi, phi, psd):
        """
        Pre-project validation waveforms to interferometer
        responses and threshold their SNRs at our minimum value.
        """

        device = psd.device
        num_batches = x_per_y(len(waveforms), self.val_batch_size)
        responses = []
        for i in range(num_batches):
            slc = slice(i * self.val_batch_size, (i + 1) * self.val_batch_size)
            params = [i[slc].to(device) for i in [dec, phi, psi]]
            response = self.projector(
                *params,
                snrs=self.hparams.snr_thresh,
                psds=psd,
                cross=waveforms[slc, 0].to(device),
                plus=waveforms[slc, 1].to(device),
            )
            responses.append(response.cpu())
        return torch.cat(responses, dim=0)

    def prepare_data(self):
        if self.hparams.data_dir.startswith("s3://"):
            bucket = self.hparams.data_dir.replace("s3://", "")
            bucket, *data_dir = self.hparams.data_dir.split(":")

            if not data_dir:
                data_dir = "tmp"
            else:
                data_dir = data_dir[0]
        else:
            return
        self.hparams.data_dir = data_dir

        client = S3Client(endpoint_url="https://s3-west.nrp-nautilus.io")
        objects = client.s3.meta.client.list_objects(Bucket=bucket)["Contents"]
        if not objects:
            raise ValueError(f"No data in S3 bucket {bucket}")

        os.mkdir(os.path.join(data_dir, "train", "background"), exist_ok=True)
        for object in objects:
            target = os.path.join(data_dir, object["Key"])
            client.s3.meta.client.download_file(bucket, object["Key"], target)

    def setup(self, stage: str) -> None:
        device = self.get_local_device()
        if not torch.distributed.is_initialized():
            world_size = 1
            rank = 0
        else:
            world_size = torch.distributed.get_world_size()
            rank = torch.distributed.get_rank()

        logger_name = "AframeDataset"
        if world_size > 1:
            logger_name += f":{rank}"
        self._logger = logging.getLogger(logger_name)
        self._logger.info(f"Inferred sample rate {self.sample_rate}")

        # move all our modules with buffers to our local device
        self.projector.to(device)
        self.whitener.to(device)

        # load in our validation background up front and
        # compute which timeslides we'll do on this device
        # if we're doing distributed training so we'll know
        # which waveforms to subsample
        self._logger.info("Loading validation background data")
        val_background = []
        with h5py.File(self.valid_fnames[0], "r") as f:
            for ifo in self.hparams.ifos:
                val_background.append(torch.Tensor(f[ifo][:]))
        val_background = torch.stack(val_background)
        self.timeslides = get_timeslides(
            val_background,
            self.hparams.valid_livetime,
            self.sample_rate,
            self.sample_length,
            self.hparams.valid_stride,
            self.val_batch_size
        )

        # calculate the validation background PSD up front
        # on the CPU then move the psd estimator to the device
        psd = self.psd_estimator.spectral_density(val_background.double())
        psd = psd.to(device)
        self.psd_estimator.to(device)

        self._logger.info("Loading waveforms")
        with h5py.File(f"{self.hparams.data_dir}/signals.h5", "r") as f:
            num_signals = len(f["signals"])
            num_valid_signals = int(self.hparams.valid_frac * num_signals)
            num_train_signals = num_signals - num_valid_signals

            # log global information only on rank 0 process
            if not rank:
                self._logger.info(
                    "Training on {} waveforms, with {} "
                    "reserved for validation".format(
                        num_train_signals, num_valid_signals
                    )
                )

            per_dev = x_per_y(num_train_signals, world_size)
            start = rank * per_dev
            stop = (rank + 1) * per_dev

            self._logger.info(f"Loading {per_dev} train waveforms")
            train_signals = torch.Tensor(f["signals"][start:stop])
            self._logger.info("Training waveforms loaded")

            # increase the likilehood of sampling waveforms
            # by a factor that accounts for the fact that
            # not all of them will be marked as signal due
            # to swapping and muting
            upweight = (
                1
                + self.hparams.swap_frac * self.hparams.mute_frac
                - self.hparams.swap_frac
                - self.hparams.mute_frac
            )
            self.waveform_sampler = aug.WaveformSampler(
                self.hparams.waveform_prob / upweight,
                cross=train_signals[:, 0],
                plus=train_signals[:, 1],
            )

            # subsample which waveforms we're using
            # based on the fraction of shifts that
            # are getting done on this device
            per_dev = x_per_y(num_valid_signals, world_size)
            start = -(rank + 1) * per_dev
            stop = -rank * per_dev or None

            # grab the appropriate slice of validation waveforms
            self._logger.info(f"Loading {per_dev} validation waveforms")
            val_signals = torch.Tensor(f["signals"][start: stop])
            val_dec = torch.Tensor(f["dec"][start: stop])
            val_psi = torch.Tensor(f["psi"][start: stop])
            val_phi = torch.Tensor(f["ra"][start: stop])

        self._logger.info("Projecting validation waveforms to IFO responses")
        # now finally project our raw polarizations into
        # inteferometer responses on this device using
        # the PSD from the entire background segment
        self.val_waveforms = self.project_val_waveforms(
            val_signals, val_dec, val_psi, val_phi, psd
        )
        self._logger.info("Initial dataloading complete")

    def on_after_batch_transfer(self, batch, _):
        if self.trainer.training:
            # if we're training, perform random augmentations
            # on input data and use it to impact labels
            [X] = batch
            batch = self.augment(X)
        elif self.trainer.validating or self.trainer.sanity_checking:
            # If we're in validation mode but we're not validating
            # on the local device, the relevant tensors will be
            # empty, so just pass them through with a 0 shift to
            # indicate that this should be ignored
            [background, _, timeslide_idx], [signals] = batch

            # If we're validating, unfold the background
            # data into a batch of overlapping kernels now that
            # we're on the GPU so that we're not transferring as
            # much data from CPU to GPU. Once everything is
            # on-device, pre-inject signals into background.
            shift = self.timeslides[timeslide_idx].shift_size
            background, signals = self.build_val_batches(background, signals)
            batch = (shift, background, signals)
        return batch

    @torch.no_grad()
    def augment(self, X: Tensor) -> tuple[Tensor, Tensor]:
        """
        Perform augmentations on a training batch
        and generate labels indicating which samples
        have had injections performed on them.
        """

        X, psds = self.psd_estimator(X)

        X = self.inverter(X)
        X = self.reverser(X)
        *params, polarizations, mask = self.waveform_sampler(X)

        N = len(params[0])
        snrs = self.snr_sampler(N).to(X.device)
        responses = self.projector(*params, snrs, psds[mask], **polarizations)
        kernels = sample_kernels(
            responses,
            kernel_size=X.size(-1),
            max_center_offset=self.pad_size,
            coincident=True,
        )

        # perform augmentations on the responses themselves,
        # keep track of which indices have been augmented
        kernels, swap_indices = self.swapper(kernels)
        kernels, mute_indices = self.muter(kernels)

        # inject the IFO responses and whiten
        X[mask] += kernels
        X = self.whitener(X, psds)

        # mark which responses got augmented
        # so that we don't mark these as signal
        idx = torch.where(mask)[0]
        mask[idx[mute_indices]] = 0
        mask[idx[swap_indices]] = 0

        # make labels
        y = torch.zeros((X.size(0), 1), device=X.device)
        y[mask] += 1
        return X, y

    @torch.no_grad()
    def build_val_batches(
        self, background: Tensor, signals: Tensor
    ) -> tuple[Tensor, Tensor]:
        """
        Unfold a timeseries of background data
        into a batch of kernels, then inject
        multiple views of the provided signals
        into these timeseries. Whiten all tensors
        and return both the background and injected
        batches.
        """

        # TODO: in the same way we do inference, should we
        # use a longer PSD length and do the whitening
        # before we do the windowing to reduce compute?
        sample_size = int(self.sample_length * self.sample_rate)
        stride = int(self.hparams.valid_stride * self.sample_rate)
        background = unfold_windows(background, sample_size, stride=stride)

        X, psd = self.psd_estimator(background)
        X_bg = self.whitener(X, psd)

        # sometimes at the end of a segment,
        # there won't be enough background
        # kernels and so we'll have to inject
        # our signals on overlapping data and
        # ditch some at the end
        step = int(len(X) / len(signals))
        if not step:
            signals = signals[: len(X)]
        else:
            X = X[::step][: len(signals)]
            psd = psd[::step][: len(signals)]

        # create `num_view` instances of the injection on top of
        # the background, each showing a different, overlapping
        # portion of the signal
        kernel_size = X.size(-1)
        center = signals.size(-1) // 2

        step = kernel_size + 2 * self.pad_size
        step /= self.hparams.num_valid_views - 1
        X_inj = []
        for i in range(self.hparams.num_valid_views):
            start = center + self.pad_size - int(i * step)
            stop = start + kernel_size
            injected = X + signals[:, :, int(start) : int(stop)]
            injected = self.whitener(injected, psd)
            X_inj.append(injected)
        X_inj = torch.stack(X_inj)
        return X_bg, X_inj

    def val_dataloader(self) -> ZippedDataset:
        background_dataset = pl.utilities.combined_loader.CombinedLoader(
            self.timeslides, mode="sequential"
        )

        # Figure out how many batches of background
        # we're going to go through, then batch the
        # signals so that they're spaced evenly
        # throughout all those batches.
        num_waveforms = len(self.val_waveforms)
        signal_batch_size = (num_waveforms - 1) // len(background_dataset) + 1
        signal_dataset = torch.utils.data.TensorDataset(self.val_waveforms)
        signal_loader = torch.utils.data.DataLoader(
            signal_dataset,
            batch_size=signal_batch_size,
            shuffle=False,
            pin_memory=False,
        )
        return ZippedDataset(background_dataset, signal_loader)

    def train_dataloader(self) -> torch.utils.data.DataLoader:
        dataset = Hdf5TimeSeriesDataset(
            self.train_fnames,
            channels=self.hparams.ifos,
            kernel_size=int(self.sample_rate * self.sample_length),
            batch_size=self.hparams.batch_size,
            batches_per_epoch=self.steps_per_epoch,
            coincident=False,
        )

        pin_memory = isinstance(
            self.trainer.accelerator, pl.accelerators.CUDAAccelerator
        )
        local_world_size = len(self.trainer.device_ids)
        num_workers = min(6, int(os.cpu_count() / local_world_size))
        dataloader = torch.utils.data.DataLoader(
            dataset,
            num_workers=num_workers,
            pin_memory=pin_memory
        )
        return dataloader
