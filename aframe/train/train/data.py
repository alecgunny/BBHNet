import glob
import logging
import os
from collections.abc import Sequence
from typing import Optional

import h5py
import lightning.pytorch as pl
import s3fs
import torch
from train import augmentations as aug
from train.validation import get_timeslides

from aframe.architectures.preprocessing import PsdEstimator
from aframe.utils import x_per_y
from ml4gw.dataloading import Hdf5TimeSeriesDataset
from ml4gw.distributions import PowerLaw
from ml4gw.transforms import Whiten
from ml4gw.utils.slicing import sample_kernels, unfold_windows

Tensor = torch.Tensor


# TODO: using this right now because
# lightning.pytorch.utilities.CombinedLoader
# is not supported when calling `.fit`. Once
# this has been fixed in
# https://github.com/Lightning-AI/lightning/issues/16830,
# we should switch to using a CombinedLoader for validation
class ZippedDataset(torch.utils.data.IterableDataset):
    def __init__(self, *datasets):
        super().__init__()
        self.datasets = datasets

    def __len__(self):
        lengths = []
        for dset in self.datasets:
            try:
                lengths.append(len(dset))
            except Exception as e:
                raise e from None
        return min(lengths)

    def __iter__(self):
        return zip(*self.datasets)


class AframeDataset(pl.LightningDataModule):
    def __init__(
        self,
        # data loading args
        data_dir: str,
        ifos: Sequence[str],
        valid_frac: float,
        # preprocessing args
        batch_size: int,
        kernel_length: float,
        fduration: float,
        psd_length: float,
        # augmentation args
        waveform_prob: float,
        swap_frac: float,
        mute_frac: float,
        snr_thresh: float = 4,
        max_snr: float = 100,
        snr_alpha: float = 3,
        trigger_pad: float = 0,
        fftlength: Optional[float] = None,
        highpass: Optional[float] = None,
        # validation args
        valid_stride: Optional[float] = None,
        num_valid_views: int = 4,
        valid_livetime: float = (3600 * 12),
    ) -> None:
        super().__init__()
        self.save_hyperparameters()
        self.num_ifos = len(ifos)

        # set up some of the modules we'll need for
        # augmentation that don't require knowing
        # the sample rate, which we'll infer at
        # data loading time from our data files
        self.inverter = aug.SignalInverter(0.5)
        self.reverser = aug.SignalReverser(0.5)
        self.snr_sampler = PowerLaw(snr_thresh, max_snr, snr_alpha)
        self.swapper = aug.ChannelSwapper(swap_frac)
        self.muter = aug.ChannelMuter(mute_frac)

        # some will require sample rate info
        self.psd_estimator = None
        self.whitener = None
        self.projector = None
        self.waveform_sampler = None

    def get_local_device(self):
        if not self.trainer.device_ids:
            return "cpu"
        elif len(self.trainer.device_ids) == 1:
            return f"cuda:{self.trainer.device_ids[0]}"
        else:
            rank = os.getenv("LOCAL_RANK", "0")
            device_id = self.trainer.device_ids[int(rank)]
            return f"cuda:{device_id}"

    def sizeify(self, length):
        return int(length * self.sample_rate)

    @property
    def sample_length(self) -> float:
        """Length of samples generated by datasets in seconds"""
        return (
            self.hparams.kernel_length
            + self.hparams.fduration
            + self.hparams.psd_length
        )

    @property
    def pad_size(self) -> int:
        """
        Number of samples away from edge of kernel to ensure
        that waveforms are injected at.
        """
        return self.sizeify(self.hparams.trigger_pad)

    # TODO: should we come up with a more clever scheme for
    # breaking up our training and validation background data?
    @property
    def train_fnames(self) -> Sequence[str]:
        fnames = glob.glob(f"{self.data_dir}/background/*.hdf5")
        return sorted(fnames)[:-1]

    @property
    def valid_fnames(self) -> Sequence[str]:
        fnames = glob.glob(f"{self.data_dir}/background/*.hdf5")
        return sorted(fnames)[-1:]

    @property
    def val_batch_size(self):
        """Use larger batch sizes when we don't need gradients."""
        return int(4 * self.hparams.batch_size)

    @torch.no_grad()
    def project_val_waveforms(self, waveforms, dec, psi, phi, psd):
        """
        Pre-project validation waveforms to interferometer
        responses and threshold their SNRs at our minimum value.
        """

        device = psd.device
        num_batches = x_per_y(len(waveforms), self.val_batch_size)
        responses = []
        for i in range(num_batches):
            slc = slice(i * self.val_batch_size, (i + 1) * self.val_batch_size)
            params = [i[slc].to(device) for i in [dec, phi, psi]]
            response = self.projector(
                *params,
                snrs=self.hparams.snr_thresh,
                psds=psd,
                cross=waveforms[slc, 0].to(device),
                plus=waveforms[slc, 1].to(device),
            )
            responses.append(response.cpu())
        return torch.cat(responses, dim=0)

    @property
    def data_dir(self):
        """
        If the data directory used to initialize the class
        is an S3 bucket, we try to resolve the directory
        to which to download the data in two ways.
        1. First, if `data_dir` has the format
           `s3://bucket-name:/target/dir`, then we'll
           create a directory `/target/dir` if it doesn't
           exist and download the data there
        2. Otherwise, if the format is just `s3://bucket-name`,
           we'll create a local directory called `tmp/` and
           download the data there.
        """
        if self.hparams.data_dir.startswith("s3://"):
            bucket = self.hparams.data_dir.replace("s3://", "")
            _, *data_dir = bucket.split(":")

            if not data_dir:
                return "tmp"
            else:
                return data_dir[0]
        else:
            return self.hparams.data_dir

    def prepare_data(self):
        """
        Download s3 data if it doesn't exist.
        """
        if self.hparams.data_dir.startswith("s3://"):
            bucket = self.hparams.data_dir.replace("s3://", "")
            bucket, *_ = bucket.split(":")
        else:
            return
        data_dir = self.data_dir
        logging.info(
            "Downloading data from S3 bucket {} to "
            "local directory {}".format(bucket, data_dir)
        )

        s3 = s3fs.S3FileSystem()
        background_dir = f"{data_dir}/background"
        os.makedirs(background_dir, exist_ok=True)
        for f in s3.glob(f"{bucket}/train/background/*.hdf5"):
            target = data_dir + f.replace(f"{bucket}/train", "")
            if not os.path.exists(target):
                logging.info(f"Downloading {f} to {target}")
                s3.download(f, target)
            else:
                logging.info(f"Object {f} already downloaded")
        
        path = "train/signals.h5"
        target = f"{data_dir}/signals.h5"
        if not os.path.exists(target):
            logging.info(f"Downloading {path} to {target}")
            s3.download(f"{bucket}/{path}", target)
        else:
            logging.info(f"Object {path} already downloaded")
        logging.info("Data download complete")

    def build_transforms(self, sample_rate: float):
        """
        Helper utility in case we ever want to construct
        this dataset on its own.
        """
        window_length = self.hparams.kernel_length + self.hparams.fduration
        fftlength = self.hparams.fftlength or window_length
        self.psd_estimator = PsdEstimator(
            window_length,
            sample_rate,
            fftlength,
            fast=self.hparams.highpass is not None,
            average="median",
        )
        self.whitener = Whiten(
            self.hparams.fduration,
            sample_rate,
            self.hparams.highpass
        )
        self.projector = aug.WaveformProjector(
            self.hparams.ifos,
            sample_rate,
            self.hparams.highpass
        )
        self.sample_rate = sample_rate

    def setup(self, stage: str) -> None:
        device = self.get_local_device()
        if not torch.distributed.is_initialized():
            world_size = 1
            rank = 0
        else:
            world_size = torch.distributed.get_world_size()
            rank = torch.distributed.get_rank()

        logger_name = "AframeDataset"
        if world_size > 1:
            logger_name += f":{rank}"
        self._logger = logging.getLogger(logger_name)

        with h5py.File(self.train_fnames[0], "r") as f:
            sample_rate = 1 / f[self.hparams.ifos[0]].attrs["dx"]
        self._logger.info(f"Inferred sample rate {sample_rate}")

        # now define some of the augmentation transforms
        # that require sample rate information
        self._logger.info("Constructing sample rate dependent transforms")
        self.build_transforms(sample_rate)

        # move all our modules with buffers to our local device
        self.projector.to(device)
        self.whitener.to(device)

        # load in our validation background up front and
        # compute which timeslides we'll do on this device
        # if we're doing distributed training so we'll know
        # which waveforms to subsample
        self._logger.info("Loading validation background data")
        val_background = []
        with h5py.File(self.valid_fnames[0], "r") as f:
            for ifo in self.hparams.ifos:
                val_background.append(torch.Tensor(f[ifo][:]))
        val_background = torch.stack(val_background)
        self.timeslides = get_timeslides(
            val_background,
            self.hparams.valid_livetime,
            self.sample_rate,
            self.sample_length,
            self.hparams.valid_stride,
            self.val_batch_size
        )

        # calculate the validation background PSD up front
        # on the CPU then move the psd estimator to the device
        psd = self.psd_estimator.spectral_density(val_background.double())
        psd = psd.to(device)
        self.psd_estimator.to(device)

        self._logger.info("Loading waveforms")
        with h5py.File(f"{self.data_dir}/signals.h5", "r") as f:
            num_signals = len(f["signals"])
            num_valid_signals = int(self.hparams.valid_frac * num_signals)
            num_train_signals = num_signals - num_valid_signals

            # log global information only on rank 0 process
            if not rank:
                self._logger.info(
                    "Training on {} waveforms, with {} "
                    "reserved for validation".format(
                        num_train_signals, num_valid_signals
                    )
                )

            per_dev = x_per_y(num_train_signals, world_size)
            start = rank * per_dev
            stop = (rank + 1) * per_dev

            self._logger.info(f"Loading {per_dev} train waveforms")
            train_signals = torch.Tensor(f["signals"][start:stop])
            self._logger.info("Training waveforms loaded")

            # increase the likilehood of sampling waveforms
            # by a factor that accounts for the fact that
            # not all of them will be marked as signal due
            # to swapping and muting
            upweight = (
                1
                + self.hparams.swap_frac * self.hparams.mute_frac
                - self.hparams.swap_frac
                - self.hparams.mute_frac
            )
            self.waveform_sampler = aug.WaveformSampler(
                self.hparams.waveform_prob / upweight,
                cross=train_signals[:, 0],
                plus=train_signals[:, 1],
            )

            # subsample which waveforms we're using
            # based on the fraction of shifts that
            # are getting done on this device
            per_dev = x_per_y(num_valid_signals, world_size)
            start = -(rank + 1) * per_dev
            stop = -rank * per_dev or None

            # grab the appropriate slice of validation waveforms
            self._logger.info(f"Loading {per_dev} validation waveforms")
            val_signals = torch.Tensor(f["signals"][start: stop])
            val_dec = torch.Tensor(f["dec"][start: stop])
            val_psi = torch.Tensor(f["psi"][start: stop])
            val_phi = torch.Tensor(f["ra"][start: stop])

        self._logger.info("Projecting validation waveforms to IFO responses")
        # now finally project our raw polarizations into
        # inteferometer responses on this device using
        # the PSD from the entire background segment
        self.val_waveforms = self.project_val_waveforms(
            val_signals, val_dec, val_psi, val_phi, psd
        )
        self._logger.info("Initial dataloading complete")

    def on_after_batch_transfer(self, batch, _):
        if self.trainer.training:
            # if we're training, perform random augmentations
            # on input data and use it to impact labels
            [X] = batch
            batch = self.augment(X)
        elif self.trainer.validating or self.trainer.sanity_checking:
            # If we're in validation mode but we're not validating
            # on the local device, the relevant tensors will be
            # empty, so just pass them through with a 0 shift to
            # indicate that this should be ignored
            [background, _, timeslide_idx], [signals] = batch

            # If we're validating, unfold the background
            # data into a batch of overlapping kernels now that
            # we're on the GPU so that we're not transferring as
            # much data from CPU to GPU. Once everything is
            # on-device, pre-inject signals into background.
            shift = self.timeslides[timeslide_idx].shift_size
            background, signals = self.build_val_batches(background, signals)
            batch = (shift, background, signals)
        return batch

    @torch.no_grad()
    def augment(self, X: Tensor) -> tuple[Tensor, Tensor]:
        """
        Perform augmentations on a training batch
        and generate labels indicating which samples
        have had injections performed on them.
        """

        X, psds = self.psd_estimator(X)

        X = self.inverter(X)
        X = self.reverser(X)
        *params, polarizations, mask = self.waveform_sampler(X)

        N = len(params[0])
        snrs = self.snr_sampler(N).to(X.device)
        responses = self.projector(*params, snrs, psds[mask], **polarizations)
        kernels = sample_kernels(
            responses,
            kernel_size=X.size(-1),
            max_center_offset=self.pad_size,
            coincident=True,
        )

        # perform augmentations on the responses themselves,
        # keep track of which indices have been augmented
        kernels, swap_indices = self.swapper(kernels)
        kernels, mute_indices = self.muter(kernels)

        # inject the IFO responses and whiten
        X[mask] += kernels
        X = self.whitener(X, psds)

        # mark which responses got augmented
        # so that we don't mark these as signal
        idx = torch.where(mask)[0]
        mask[idx[mute_indices]] = 0
        mask[idx[swap_indices]] = 0

        # make labels
        y = torch.zeros((X.size(0), 1), device=X.device)
        y[mask] += 1
        return X, y

    @torch.no_grad()
    def build_val_batches(
        self, background: Tensor, signals: Tensor
    ) -> tuple[Tensor, Tensor]:
        """
        Unfold a timeseries of background data
        into a batch of kernels, then inject
        multiple views of the provided signals
        into these timeseries. Whiten all tensors
        and return both the background and injected
        batches.
        """

        # TODO: in the same way we do inference, should we
        # use a longer PSD length and do the whitening
        # before we do the windowing to reduce compute?
        sample_size = self.sizeify(self.sample_length)
        stride = self.sizeify(self.hparams.valid_stride)
        background = unfold_windows(background, sample_size, stride=stride)

        X, psd = self.psd_estimator(background)
        X_bg = self.whitener(X, psd)

        # sometimes at the end of a segment,
        # there won't be enough background
        # kernels and so we'll have to inject
        # our signals on overlapping data and
        # ditch some at the end
        step = int(len(X) / len(signals))
        if not step:
            signals = signals[: len(X)]
        else:
            X = X[::step][: len(signals)]
            psd = psd[::step][: len(signals)]

        # create `num_view` instances of the injection on top of
        # the background, each showing a different, overlapping
        # portion of the signal
        kernel_size = X.size(-1)
        center = signals.size(-1) // 2

        step = kernel_size + 2 * self.pad_size
        step /= self.hparams.num_valid_views - 1
        X_inj = []
        for i in range(self.hparams.num_valid_views):
            start = center + self.pad_size - int(i * step)
            stop = start + kernel_size
            injected = X + signals[:, :, int(start) : int(stop)]
            injected = self.whitener(injected, psd)
            X_inj.append(injected)
        X_inj = torch.stack(X_inj)
        return X_bg, X_inj

    def val_dataloader(self) -> ZippedDataset:
        background_dataset = pl.utilities.combined_loader.CombinedLoader(
            self.timeslides, mode="sequential"
        )
        iter(background_dataset)

        # Figure out how many batches of background
        # we're going to go through, then batch the
        # signals so that they're spaced evenly
        # throughout all those batches.
        num_waveforms = len(self.val_waveforms)
        signal_batch_size = (num_waveforms - 1) // len(background_dataset) + 1
        signal_dataset = torch.utils.data.TensorDataset(self.val_waveforms)
        signal_loader = torch.utils.data.DataLoader(
            signal_dataset,
            batch_size=signal_batch_size,
            shuffle=False,
            pin_memory=False,
        )
        return ZippedDataset(background_dataset, signal_loader)

    def train_dataloader(self) -> torch.utils.data.DataLoader:
        num_waveforms = self.waveform_sampler.num_waveforms
        waveforms_per_batch = (
            self.hparams.batch_size * self.hparams.waveform_prob
        )
        steps_per_epoch = int(4 * num_waveforms / waveforms_per_batch)

        dataset = Hdf5TimeSeriesDataset(
            self.train_fnames,
            channels=self.hparams.ifos,
            kernel_size=self.sizeify(self.sample_length),
            batch_size=self.hparams.batch_size,
            batches_per_epoch=steps_per_epoch,
            coincident=False,
        )

        pin_memory = isinstance(
            self.trainer.accelerator, pl.accelerators.CUDAAccelerator
        )
        local_world_size = len(self.trainer.device_ids)
        num_workers = min(6, int(os.cpu_count() / local_world_size))
        dataloader = torch.utils.data.DataLoader(
            dataset,
            num_workers=num_workers,
            pin_memory=pin_memory
        )
        return dataloader
